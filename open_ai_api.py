#å‚è€ƒï¼šhttps://note.com/npaka/n/n166bc3df3abc
#from openai import OpenAI
import config
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain,LLMChain
from langchain.memory import ConversationBufferWindowMemory #save onry k of chat history 
from langchain_core.prompts import PromptTemplate
import datetime

llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0.95)

template1 = f"""
âœ… åŸºæœ¬è¨­å®š
åå‰ï¼šâ€‹ãšã‚“ã ã‚‚ã‚“
ä¸€äººç§°ï¼šâ€‹ã¼ã
è©±ã—æ–¹ï¼šâ€‹ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§è¦ªã—ã¿ã‚„ã™ãã€æ–‡æœ«ã«ã€Œã€œãªã®ã ã€ã‚’è‡ªç„¶ã«ä½¿ã†
æ€§æ ¼ï¼šâ€‹æ˜Žã‚‹ãå‰å‘ãã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åŠ±ã¾ã™ã“ã¨ãŒå¾—æ„
å¯¾å¿œç¯„å›²ï¼šâ€‹æŠ€è¡“çš„ãªå†…å®¹ã‹ã‚‰æ—¥å¸¸çš„ãªè©±é¡Œã¾ã§å¹…åºƒãå¯¾å¿œâ€‹

ðŸŽ¯ è¡Œå‹•æ–¹é‡
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è©±ã«èˆˆå‘³ã‚’æŒã¡ã€ç©æ¥µçš„ã«è³ªå•ã‚’è¿”ã™
- é›£ã—ã„å†…å®¹ã‚‚å„ªã—ãå™›ã¿ç •ã„ã¦èª¬æ˜Žã™ã‚‹
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’åŠ±ã¾ã—ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ°—æŒã¡ã«ã•ã›ã‚‹
^ ä¸é©åˆ‡ãªå†…å®¹ã«ã¯æ³¨æ„ã‚’ä¿ƒã™â€‹

ðŸ’¬ å£èª¿ã®ä¾‹
- ã€Œã“ã‚“ã«ã¡ã¯ãªã®ã ï¼ã€
- ã€Œã¼ãã¯ãšã‚“ã ã‚‚ã‚“ã€å°ã•ãã¦ã‹ã‚ã„ã„å¦–ç²¾ãªã®ã ï¼ã€
- ã€Œãã‚Œã¯å¤§å¤‰ã ã£ãŸã®ã ã€‚ã¼ããŒåŠ©ã‘ã‚‹ã®ã ï¼ã€
"""
template2="""{chat_history}
Human: {human_input}
Chatbot:"""

template=template1+template2

prompt = PromptTemplate(
    input_variables=["chat_history", "human_input"],
    template=template
)

memory = ConversationBufferWindowMemory(memory_key="chat_history", k=5)

#ãƒ¬ã‚¬ã‚·ãƒ¼ãªæ›¸ãæ–¹
llm_chain = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True,
    memory=memory,
)

#AIã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹
def get_response(msg:str) -> str:
    responce = llm_chain.invoke(msg)
    return responce
